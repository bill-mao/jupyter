{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 128个文件 - 639101 条新闻； 4992.9765625 平均"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T15:19:05.472893Z",
     "start_time": "2018-12-02T15:18:55.518648Z"
    }
   },
   "outputs": [],
   "source": [
    "# 选择抽取一部分数据进行测试\n",
    "FILE_NUM = 999\n",
    "\n",
    "import os \n",
    "files = [i for i in os.listdir() if i.endswith('txt') and i.startswith('news')]\n",
    "news = []\n",
    "for f in files[:FILE_NUM]:\n",
    "    # 数据集的格式是GBK； 用GBK的超级GB18030 来解码就不会出现解码问题\n",
    "    with open(f, 'r', encoding = 'GB18030') as file:\n",
    "        # print(f)\n",
    "        news.append(file.read())\n",
    "        # print(news[-1][:222:333])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T15:19:57.398390Z",
     "start_time": "2018-12-02T15:19:16.695733Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://2008.163.com/comment/0074/2008_bbs/04/4CFRUM0400742437.html', 'http://news.qq.com/a/20080620/000677_6.htm', 'http://2008.qq.com/a/20080620/000288_23.htm?', 'http://edu.sina.com.cn/shiti/2008/0612/164307331.html', 'http://sports.sina.com.cn/z/pl08_summer/photo/61919/index.shtml']\n",
      "['013e65ba3b398a67-b4f5d9a362314a50', '005323975d05d5a5-309908a255a73ab0', '01dedef2bec15cfe-007908a255a73ab0', '012629b4fb41324e-4c207783d4cca6e0', '0194cc351d87bc5b-63207783d4cca6e0']\n",
      "['', '', '', '２００８年高等学校全国统一考试数学文科试题（上海卷）', '英超']\n",
      "['', '组图：震前汶川风光\\ue40c震前汶川风光\\u3000ＱＱ群４９１４６６７．作者肚螂皮', '', '＜＜题目类别选择\\ue40c一、填空题\\ue40c二、选择题\\ue40c三、计算题', '']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['http://2008.163.com/comment/0074/2008_bbs/04/4CFRUM0400742437.html',\n",
       " 'http://news.qq.com/a/20080620/000677_6.htm']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 解析XML文件，初步获取数据\n",
    "# url, id, title, content\n",
    "# <doc> <url> <docno> <contenttitle> <content>\n",
    "import re\n",
    "regexs = [r'<doc>([\\s\\S]*?)</doc>', r'<url>([\\s\\S]*?)</url>',r'<docno>([\\s\\S]*?)</docno>',\n",
    "          r'<contenttitle>([\\s\\S]*?)</contenttitle>',r'<content>([\\s\\S]*?)</content>',]\n",
    "regexs = [ re.compile(r) for r in regexs]\n",
    "\n",
    "# 列模式矩阵存储，第一列 url， id， title， content， label， 这样\n",
    "li_news = [[] for i in range(len(regexs)-1)]\n",
    "for s in news:\n",
    "    dociter = regexs[0].finditer(s)\n",
    "    for d in dociter:\n",
    "        # print(d.group())\n",
    "        for i in range(1,len(regexs)):\n",
    "            # print(regexs[i].search(d.group()))\n",
    "            # print(regexs[i].search(d.group()).groups(1))\n",
    "            # raise Exception\n",
    "            li_news[i-1].append(regexs[i].search(d.group()).groups(1)[0])\n",
    "for i in li_news:\n",
    "    print(i[:5])\n",
    "\n",
    "li_news[0][:2]\n",
    "# 可以看出数据标题，内容都有可能有缺失值，所以之后要处理这个问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-01T00:11:08.252148Z",
     "start_time": "2018-12-01T00:11:07.668595Z"
    }
   },
   "outputs": [],
   "source": [
    "# 获取分类的label -- url http头后面的第一个短语\n",
    "reg_label = re.compile(r'//(\\w*?)\\.')\n",
    "labels = []\n",
    "for url in li_news[0]:\n",
    "    label = reg_label.search(url)\n",
    "    if label:\n",
    "        labels.append(label.groups()[0])\n",
    "    else:\n",
    "        labels.append('')\n",
    "li_news.append(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-01T00:11:33.414876Z",
     "start_time": "2018-12-01T00:11:33.327111Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sports', 134975),\n",
       " ('finance', 129299),\n",
       " ('news', 74209),\n",
       " ('ent', 62813),\n",
       " ('auto', 52561),\n",
       " ('2008', 36639),\n",
       " ('edu', 34393),\n",
       " ('lady', 33159),\n",
       " ('money', 24831),\n",
       " ('eladies', 14116),\n",
       " ('tech', 10975),\n",
       " ('house', 7167),\n",
       " ('military', 5932),\n",
       " ('health', 4739),\n",
       " ('culture', 4040),\n",
       " ('mil', 3516),\n",
       " ('fun', 2222),\n",
       " ('china', 1642),\n",
       " ('cul', 920),\n",
       " ('www', 405),\n",
       " ('meirong', 222),\n",
       " ('caifu', 185),\n",
       " ('sina', 133),\n",
       " ('war', 4),\n",
       " ('tour', 4)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(labels).most_common(100)\n",
    "# 可以看出数据有严重的不平衡，这样子训练的结果会有很大的偏差，需要对训练集进行一定的处理：增大稀疏类的权重或者过抽样\n",
    "# 并且标题不同命但是同类别的，eg finance 和 money； military 和 war； culture 和cul "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-01T00:13:19.722947Z",
     "start_time": "2018-12-01T00:13:19.716963Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "639101"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)\n",
    "# 一共有639101条数据，很充分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-01T01:01:38.800877Z",
     "start_time": "2018-12-01T01:01:02.691407Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_news = pd.DataFrame(li_news)\n",
    "df_news = df_news.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-01T01:09:17.108676Z",
     "start_time": "2018-12-01T01:09:17.103689Z"
    }
   },
   "outputs": [],
   "source": [
    "dic_merge = {'money' :'finance',\n",
    "    'eladies': 'lady',\n",
    "    'mil': 'military'}\n",
    "set_del = {'auto','2008','fun','china','cul','www','meirong','caifu','sina','war','tour'}\n",
    "# df_news[]\n",
    "    \n",
    "\n",
    "# topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-01T02:00:17.488149Z",
     "start_time": "2018-12-01T02:00:17.343300Z"
    }
   },
   "outputs": [],
   "source": [
    "df_news = df_news[ ~df_news[4].isin(set_del)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-01T01:57:24.366222Z",
     "start_time": "2018-12-01T01:57:24.354254Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>auto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>auto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       4\n",
       "0   2008\n",
       "2   2008\n",
       "5   auto\n",
       "7   auto\n",
       "14  2008"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp[tmp.isin(set_del)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-01T02:03:02.809969Z",
     "start_time": "2018-12-01T02:03:02.346847Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_news.iloc[:, [4]].replace(dic_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-01T02:07:51.866317Z",
     "start_time": "2018-12-01T02:07:36.714026Z"
    }
   },
   "outputs": [],
   "source": [
    "df_news.to_csv('processed.csv')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index, 0<url> 1<docno> 2<contenttitle> 3<content>, 4label\n",
    "# ,0,1,2,3,4 \n",
    "# 1,http://news.qq.com/a/20080620/000677_6.htm,005323975d05d5a5-309908a255a73ab0,,组图：震前汶川风光震前汶川风光　ＱＱ群４９１４６６７．作者肚螂皮,news "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-01T02:27:36.538718Z",
     "start_time": "2018-12-01T02:27:36.490848Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable          Type                          Data/Info\n",
      "---------------------------------------------------------\n",
      "Counter           type                          <class 'collections.Counter'>\n",
      "FILE_NUM          int                           999\n",
      "NamespaceMagics   MetaHasTraits                 <class 'IPython.core.magi<...>mespace.NamespaceMagics'>\n",
      "d                 Match                         <re.Match object; span=(4<...>163.com/08/0618/08/4EN5H>\n",
      "df_news           DataFrame                                              <...>[544164 rows x 5 columns]\n",
      "dic_merge         dict                          n=3\n",
      "dociter           callable_iterator             <callable_iterator object at 0x000001AAB5D97A90>\n",
      "f                 str                           news.allsites.980806.txt\n",
      "file              TextIOWrapper                 <_io.TextIOWrapper name='<...>e='r' encoding='GB18030'>\n",
      "files             list                          n=128\n",
      "get_ipython       function                      <function get_ipython at 0x000001AAAD6A5488>\n",
      "getsizeof         builtin_function_or_method    <built-in function getsizeof>\n",
      "i                 bool                          False\n",
      "json              module                        <module 'json' from 'c:\\\\<...>\\lib\\\\json\\\\__init__.py'>\n",
      "label             Match                         <re.Match object; span=(5, 13), match='//money.'>\n",
      "labels            list                          n=639101\n",
      "li_news           list                          n=5\n",
      "news              list                          n=128\n",
      "np                module                        <module 'numpy' from 'c:\\<...>ges\\\\numpy\\\\__init__.py'>\n",
      "os                module                        <module 'os' from 'c:\\\\pr<...>s\\\\python37\\\\lib\\\\os.py'>\n",
      "pd                module                        <module 'pandas' from 'c:<...>es\\\\pandas\\\\__init__.py'>\n",
      "re                module                        <module 're' from 'c:\\\\pr<...>s\\\\python37\\\\lib\\\\re.py'>\n",
      "reg_label         Pattern                       re.compile('//(\\\\w*?)\\\\.')\n",
      "regexs            list                          n=5\n",
      "s                 str                           <doc>\\n<url>http://ent.si<...>０万港元。</content>\\n</doc>\\n\n",
      "set_del           set                           {'sina', 'www', 'meirong'<...>, 'china', '2008', 'war'}\n",
      "tmp               DataFrame                               4\\n0      2008\\<...>n31   sports\\n32  finance\n",
      "url               str                           http://money.163.com/08/0<...>/08/4EN5HHME00251RJ2.html\n",
      "var_dic_list      function                      <function var_dic_list at 0x000001AAB5CF1378>\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-01T02:29:40.197437Z",
     "start_time": "2018-12-01T02:29:40.071723Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "del news\n",
    "del li_news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T15:29:04.835594Z",
     "start_time": "2018-12-02T15:28:49.593629Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>组图：震前汶川风光震前汶川风光　ＱＱ群４９１４６６７．作者肚螂皮</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>＜＜题目类别选择一、填空题二、选择题三、计算题</td>\n",
       "      <td>edu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>评论：１北京时间６月４日，捷克国家足球队离开奥地利的因斯布鲁克训练基地，赶赴瑞士准备与东道...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>０．０００　（０．０００％）　昨收盘：０．０００　今开盘：０．０００　最高价：０．０００　最...</td>\n",
       "      <td>finance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   3        4\n",
       "0                  组图：震前汶川风光震前汶川风光　ＱＱ群４９１４６６７．作者肚螂皮     news\n",
       "1                         ＜＜题目类别选择一、填空题二、选择题三、计算题      edu\n",
       "2                                                NaN   sports\n",
       "3  评论：１北京时间６月４日，捷克国家足球队离开奥地利的因斯布鲁克训练基地，赶赴瑞士准备与东道...   sports\n",
       "4  ０．０００　（０．０００％）　昨收盘：０．０００　今开盘：０．０００　最高价：０．０００　最...  finance"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# df_news = pd.read_csv('processed.csv', index_col = 0, usecols=[  '3', '4'])\n",
    "df_news = pd.read_csv('processed.csv',  usecols=[  '3', '4'])\n",
    "# df_news = pd.read_csv('processed.csv', index_col = 0, usecols=[ np.nan, '3', '4'])\n",
    "df_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T15:29:04.982210Z",
     "start_time": "2018-12-02T15:29:04.874488Z"
    }
   },
   "outputs": [],
   "source": [
    "df_news = df_news[df_news['3'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T15:29:05.047028Z",
     "start_time": "2018-12-02T15:29:05.020100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'culture',\n",
       " 'edu',\n",
       " 'ent',\n",
       " 'finance',\n",
       " 'health',\n",
       " 'house',\n",
       " 'lady',\n",
       " 'military',\n",
       " 'news',\n",
       " 'sports',\n",
       " 'tech'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = set(df_news['4'] )\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T15:29:05.636054Z",
     "start_time": "2018-12-02T15:29:05.085924Z"
    }
   },
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "for l in list(labels):\n",
    "    dfs[l] = df_news.loc[df_news['4'] == l, ['3']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T15:29:05.767789Z",
     "start_time": "2018-12-02T15:29:05.760808Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['military', 'ent', 'house', 'news', 'sports', 'edu', 'finance', 'tech', 'culture', 'lady', 'health'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T15:29:05.902930Z",
     "start_time": "2018-12-02T15:29:05.895950Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124177"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfs['sports'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T15:29:06.095648Z",
     "start_time": "2018-12-02T15:29:06.083681Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>评论：１北京时间６月４日，捷克国家足球队离开奥地利的因斯布鲁克训练基地，赶赴瑞士准备与东道...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>国籍：俄罗斯性别：女生日：１９７８．４．１６身高：１．７３米体重：５９公斤项目：田...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>这是６月２８日拍摄的已竣工的国家体育场。当日，国家体育场（鸟巢）举行落成典礼。国家体育场是２...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    3\n",
       "3   评论：１北京时间６月４日，捷克国家足球队离开奥地利的因斯布鲁克训练基地，赶赴瑞士准备与东道...\n",
       "7   国籍：俄罗斯性别：女生日：１９７８．４．１６身高：１．７３米体重：５９公斤项目：田...\n",
       "14  这是６月２８日拍摄的已竣工的国家体育场。当日，国家体育场（鸟巢）举行落成典礼。国家体育场是２..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['sports'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T15:29:06.248243Z",
     "start_time": "2018-12-02T15:29:06.234281Z"
    }
   },
   "outputs": [],
   "source": [
    "del df_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T15:29:06.402829Z",
     "start_time": "2018-12-02T15:29:06.396843Z"
    }
   },
   "outputs": [],
   "source": [
    "# 每类抽取5000条新闻\n",
    "for k in dfs:\n",
    "    dfs[k] = dfs[k][:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T15:29:06.731947Z",
     "start_time": "2018-12-02T15:29:06.719979Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>评论：１北京时间６月４日，捷克国家足球队离开奥地利的因斯布鲁克训练基地，赶赴瑞士准备与东道...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>国籍：俄罗斯性别：女生日：１９７８．４．１６身高：１．７３米体重：５９公斤项目：田...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>这是６月２８日拍摄的已竣工的国家体育场。当日，国家体育场（鸟巢）举行落成典礼。国家体育场是２...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>退场，杜伊和福拉多难掩失意。（新华）【本报天津６月１４日电】（记者　舒桂林）出席新闻发布会...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>您所在的位置：　腾讯首页　＞　２００８欧锦赛　＞　＞　美女花边　＞　正文第１２３４５页（图...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    3\n",
       "3   评论：１北京时间６月４日，捷克国家足球队离开奥地利的因斯布鲁克训练基地，赶赴瑞士准备与东道...\n",
       "7   国籍：俄罗斯性别：女生日：１９７８．４．１６身高：１．７３米体重：５９公斤项目：田...\n",
       "14  这是６月２８日拍摄的已竣工的国家体育场。当日，国家体育场（鸟巢）举行落成典礼。国家体育场是２...\n",
       "22  退场，杜伊和福拉多难掩失意。（新华）【本报天津６月１４日电】（记者　舒桂林）出席新闻发布会...\n",
       "26  您所在的位置：　腾讯首页　＞　２００８欧锦赛　＞　＞　美女花边　＞　正文第１２３４５页（图..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(dfs['sports']))\n",
    "dfs['sports'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T15:29:11.120872Z",
     "start_time": "2018-12-02T15:29:11.115884Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "评论：１北京时间６月４日，捷克国家足球队离开奥地利的因斯布鲁克训练基地，赶赴瑞士准备与东道主的揭幕战。以上是相关视频报道。\n",
      "国籍：俄罗斯性别：女生日：１９７８．４．１６身高：１．７３米体重：５９公斤项目：田径（七项全能）辉煌战绩普洛科霍洛娃是俄罗斯最优秀的女子七项全能选手，她目前排名世界第二位，她的个人最好成绩是６７６５分。历史战绩２００４年：高兹斯田径赛七项全能第４名２００３年：世界田径锦标赛七项全能第４名；欧洲杯田径赛七项全能冠军２００２年：欧洲室内田径锦标赛五项全能冠军２００１年：世界田径锦标赛七项全能冠军；世界室内田径锦标赛五项全能亚军２０００年：悉尼奥运会七项全能亚军\n",
      "这是６月２８日拍摄的已竣工的国家体育场。当日，国家体育场（鸟巢）举行落成典礼。国家体育场是２００８年第２９届夏季奥运会主体育场，位于奥林匹克公园内，建筑面积２５．８万平方米，赛时坐席９．１万个，北京奥运会的开闭幕式、田径比赛和足球决赛将在此举行。新华社记者罗晓光摄网页不支持Ｆｌａｓｈ\n"
     ]
    }
   ],
   "source": [
    "for i in dfs['sports']['3'][:3]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T15:31:54.962414Z",
     "start_time": "2018-12-02T15:29:15.305254Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\coral\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.044 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import re\n",
    "\n",
    "# a=jieba.analyse.extract_tags(sentence, topK = 20, withWeight = False, allowPOS = ())\n",
    "with open('stopwords_chs.txt','r', encoding = 'utf-8') as f:\n",
    "    stopset = set([l.strip() for l in f])\n",
    "    # print(stopset)\n",
    "# 删除数字，标点符号\n",
    "regex_d = re.compile(r'^(\\W|\\d)+$')\n",
    "dic_segs = {}\n",
    "for k in dfs:\n",
    "    segs = []\n",
    "    for c in dfs[k]['3']:\n",
    "        seg = [w for w in jieba.cut(c, cut_all = False) if w not in stopset and not regex_d.match(w) and len(w) > 1]\n",
    "        segs.append(seg)\n",
    "        # print(seg)\n",
    "        # break\n",
    "    dic_segs[k] = segs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T15:31:58.218602Z",
     "start_time": "2018-12-02T15:31:57.708969Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "military 5000 ['客服', '电话', '空军', '论坛', '中国', '飞机', '西方', '差距', '真的', '很大', '阅读', '回复', '精华帖', '发主帖数', '累积', '积分', '等级', '读报', '有感', '千万', '浮躁', '中新社', '北京', '三月', '三日', '新任', '全国政协', '委员', '中国航空工业第一集团公司', '第一', '飞机', '设计', '研究院', '院长', '即将', '成立', '飞机', '公司', '出任', '设计师', '光辉', '接受', '中新社', '记者', '专访', '时说', '世界', '先进', '水平', '差距', '不太大', '有人', '十年', '五年', '国家', '扶持', '十到', '二十年', '肯定', '迎头', '赶超', '光辉', '誓言', '中国', '飞机', '赶超', '波音', '空客', '这次', '国家', '研制', '大客机', '相当于', '大小', '单通道', '中短程', '民航', '干线', '客机', '国内', '航空工业', '现实', '水平', '来看', '这次', '配套', '飞机', '发动机', '机载设备', '不得不', '暂时', '采用', '国外', '公司', '产品', '十年', '顺顺利利', '符合', '民航', '规范', '个机', '壳子', '设计', '制造', '一大', '成就', '只不过', '比上', '世纪', '七十年代', '波音', '空客', '公司', '技术水平', '一点', '研制', '水平', '差距', '光辉', '委员', '所说', '也许', '加上', '合适', '至少', '低于', '承认', '差距', '国人', '特别', '航空界', '同行', '来说', '痛苦', '飞机', '研究院', '院长', '内行', '人士', '院长', '差距', '结论', '面对现实', '承认现实', '过于', '浮躁', '浮夸', '即将', '研制', '飞机', '不利', '影响', '未来', '飞机', '设计师', '浮夸', '人祸', '院长', '幼稚', '一词', '形容', '当下', '中国', '民用航空', '工业', '确切', '别忘了', '世纪', '五十年代', '提过', '十五年', '英超', '亩产', '万斤', '豪言壮语', '不说', '航空工业', '来说', '至今', '英国', '差距', '不可否认', '不用说', '美国', '差距', '浮夸', '之风', '千万', '要不得', '浮躁', '之心', '败家', '之源', '有句', '俗语说', '只怕', '航空工业', '主管部门', '决策', '失误', '运十', '至少', '缩短', '十五年', '基础', '中国', '民用航空', '工业', '原地', '足足', '踏步', '中国', '波音', '空客', '差距', '反而', '拉大', '三十年', '形体', '难堪', '落后', '局面', '但愿', '浮夸', '浮躁', '多点', '实事求是', '作风', '国于民', '必将会', '祸国殃民', '聆听', '上帝', '惩戒', '祸国殃民', '矮子', '中国', '国防', '最少', '倒退', '社会', '矛盾', '两极', '激烈', '怀恋', '毛主席', '支持', '胡总', '温总', '新浪', '论坛', '发帖', '须知', '论坛', '论坛', '在线', '服务', '版面', '情况', '遵守', '在线', '服务', '三个', '论坛', '新手', '招募', '申请', '投诉', '建议', '守则', '自觉遵守', '爱国', '守法', '自律', '真实', '文明', '原则', '尊重', '网上', '道德', '遵守', '全国人大常委会', '维护', '互联网安全', '中华人民共和国', '各项', '法律法规', '严禁', '发表', '危害', '国家', '破坏', '民族团结', '破坏', '国家', '宗教政策', '破坏', '社会', '稳定', '侮辱', '诽谤', '教唆', '淫秽', '内容', '作品', '承担', '间接', '导致', '民事或', '刑事法律', '责任', '新浪网', '中文论坛', '栏目', '版主', '有权', '论坛', '规则', '保留', '删除', '其管辖', '论坛', '内容', '有权', '发贴', '用户', '包括', '禁言', '警告', '新浪', '论坛', '发表', '作品', '新浪网', '有权', '网站', '免费', '转载', '引用', '全国', '青少年', '网络', '文明', '公约']\n",
      "ent 5000 ['新浪', '娱乐', '赵浚凯', '执导', '尤勇', '李幼斌', '何政军', '主演', '电视', '连续剧', '沧海', '近日', '北京', '开机', '金门', '海战', '失利', '中国', '高层', '下定', '海军', '建设', '决心', '未能', '参战', '本利', '纪念', '牺牲', '战友', '李山魁', '改名', '王山魁', '医生', '许锦云成', '夫妻', '郑福德', '网页', '支持']\n",
      "house 5000 ['打造', '时尚', '建材', '科宝', '橱柜', '作品名称', '上传者', '橱柜', '实木', '复合地板', '品牌', '科宝', '橱柜', '点击', '详细', '网友', '评论', '姓名', '电话', '内容', '提交', '重置', '主编', '孙华莹', '编辑', '丽丽', '安燕', '许有', '黄巍', '战玉全', '主编', '邮箱', '广告', '招商', '热线']\n",
      "news 5000 ['组图', '震前', '汶川', '风光', '震前', '汶川', '风光', '作者', '肚螂皮']\n",
      "sports 5000 ['评论', '北京', '时间', '捷克', '国家足球队', '离开', '奥地利', '因斯布鲁克', '训练', '基地', '赶赴', '瑞士', '东道主', '揭幕战', '相关', '视频', '报道']\n",
      "edu 5000 ['题目', '类别', '选择', '填空题', '选择题', '计算题']\n",
      "finance 5000 ['收盘', '开盘', '最高价', '最低价', '成交额', '成交量', '买入价', '卖出价', '市盈率', '收益率', '最低', '分钟', '延迟', '资讯', '分时图', '线周', '读取', '行情', '资讯', '财华', '控股', '有限公司', '提供', '财华', '免责', '声明']\n",
      "tech 5000 ['一方', '拥有', '强大', '本地', '政府', '资源', '一方', '急需', '开疆拓土', '不惜', '出让', '子公司', '控股权', '代价', '占领', '市常', '江中', '药业', '公告', '公司', '出资', '万元', '受让', '九州', '集团', '持有', '江西', '九州', '公司', '股权', '增资', '控股', '江西', '九州', '公司', '增资', '万元', '最终', '达成', '江西', '九州', '控股', '这是', '合作', '江西', '九州', '经营', '管理', '九州', '集团', '负责', '九州', '集团', '总经理', '刘兆年', '记者', '江中', '药业', '首次', '大规模', '医药', '商业', '领域', '九州', '集团', '第一次', '工业', '企业', '资本', '合作', '九州', '借力', '外部', '资金', '至今', '江西', '九州', '通立', '征地', '基本建设', '预计', '工程', '完工', '投入', '运营', '截至', '去年底', '江西', '九州', '资产', '总额', '万元', '负债', '总额', '万元', '预计', '本次', '投资', '江西', '九州', '公司', '正式', '投入', '运营', '有望', '销售收入', '亿元', '力争', '收入', '亿元', '江西', '九州', '通是', '九州', '集团', '全资', '子公司', '注册资本', '万元', '一家', '即将', '投入', '运营', '公司', '九州', '集团', '集团', '出让', '控股权', '江西省', '此前', '九州', '集团', '并未', '涉足', '区域', '这是', '合作', '刘兆年', '告诉', '记者', '公司', '发展', '利用', '自有', '资金', '旗下', '公司', '全资', '子公司', '利用', '外部', '资金', '公司', '扩张', '进程', '进一步', '加速', '这次', '江中', '药业', '合作', '本来', '业务', '合作', '对方', '江西', '本地', '企业', '拥有', '本地', '资源优势', '牵手', '江中', '对方', '注入', '资金', '管理', '运营', '依然', '九州', '负责', '知情', '人士', '告诉', '记者', '控股权', '江中', '药业', '控股', '上市公司', '并表', '中国', '医药', '商业', '领域', '九州', '一匹', '黑马', '这家', '民营企业', '独特', '快批', '模式', '短短', '年内', '销售额', '亿元', '全国', '医药', '流通', '行业', '前三甲', '一家', '医药', '商业', '药品', '批发', '物流配送', '零售', '连锁', '医药', '电子商务', '核心', '业务', '企业', '集团', '九州', '下属', '全资', '医药', '子公司', '分布', '湖北', '北京', '河南', '新疆', '广东', '上海', '山东', '福建', '江苏', '重庆', '值得一提的是', '去年', '九州', '集团', '万美元', '私募', '股权', '投资', '投资方', '包括', '日本', '伊藤忠', '商事', '株式会社', '日本', '东邦', '药业', '荷兰', '发展', '银行', '香港', '惠发', '基金', '投资方', '九州', '集团', '计划', '全国', '建立', '子公司', '二级', '配送', '中心', '连锁', '零售店', '销售收入', '亿元', '庞大', '计划', '单单', '九州', '集团', '自有', '资金', '难以完成', '江中', '药业', '医药', '商业', '领域', '公告', '显示', '此次', '江中', '药业', '受让', '股权', '增资', '扩股', '投资', '万元', '江西', '九州', '控股权', '江中', '药业', '此次', '对外', '投资', '目的', '在于', '进一步', '区域', '经销商', '紧密', '合作', '推动', '江中', '药业', '产品', '商业渠道', '拓展', '市场', '分额', '事实上', '去年', '江中', '药业', '医药', '商业', '领域', '寻求', '合适', '合资', '对象', '公司', '董事会', '授权', '董事长', '易敏', '寻求', '区域', '较强', '商业', '竞争', '实力', '终端', '控制力', '企业', '合资', '合作', '投资', '金额', '超过', '人民币', '万元', '公司', '自有', '资金', '充裕', '希望', '寻找', '投资', '项目', '医药', '商业', '不错', '方向', '一位', '公司', '人士', '国内', '领域', '龙头企业', '江中', '药业', '此次', '投资', '并购', '前景', '乐观', '公司', '年报', '来看', '江中', '健胃', '消食片', '复方', '草珊瑚', '含片', '代表', '中成药', '销售收入', '超过', '亿元', '江中', '药业', '总营收', '公司', '旗下', '中医', '江中', '小舟', '恒生', '贸易', '三家', '专业化', '贸易', '公司', '负责', '药品', '批发', '销售', '事实上', '九州', '江中', '药业', '合作', '基础', '九州', '通是', '江中', '药业', '单一', '销售量', '客户', '去年', '九州', '销售', '药品', '金额', '超过', '万元', '业内人士', '指出', '我国', '医药', '商业', '市场', '寡头', '垄断', '方向', '演变', '趋势', '医药', '商业', '企业', '沟通', '合作', '医药', '生产', '企业', '不得不', '毕竟', '医药', '生产', '企业', '流通', '企业', '之间', '整合', '构筑', '一体化', '物流', '网络', '赢得', '竞争', '优势', '江中', '药业', '董秘', '吴伯帆', '江中', '药业', '对外', '投资', '一贯', '谨慎', '这次', '选择', '行业', '强者', '合作', '带来', '短期', '销售', '利好', '公司', '看重', '未来', '九州', '扩张', '预期', '流通领域', '主导', '前提', '江中', '药业', '有望', '普药', '品牌', '大批量', '定制', '生产', '领域', '工商', '合营', '模式', '探索', '江中', '药业', '内部', '最初', '质疑', '声音', '吴伯帆', '公司', '未来', '主业', '位于', '药品', '制造', '研发', '重心', '放到', '流通领域', '刘兆年', '告诉', '记者', '这是', '公司', '旗下', '第一家', '合资', '子公司', '公司', '生产', '企业', '合作', '在于', '业务', '层面', '这是', '公司', '第一次', '生产', '企业', '资本', '联合', '版权所有', '未经', '书面', '授权', '形式', '转载', '摘编']\n",
      "culture 4027 ['中华网', '搜索', '资讯', '书画', '赏析', '合作', '电话']\n",
      "lady 5000 ['夏天', '马上', '要来', '指甲油', '华丽', '登场', '华丽', '甲油', '大团圆', '遗传', '老妈', '购物狂', '基因', '很多', '乱七八糟', '心血来潮', '一堆', '回家', '这次', '收拾', '东西', '发现', '好多', '抽屉', '东西', '晒太阳', '先来', '集体照']\n",
      "health 4738 ['精彩', '专题', '热点', '排行', '今日', '报道', '行业', '要闻', '有问必答', '老年', '健康', '两性', '健康', '疾病', '健康', '妇科', '健康', '图库', '健身运动', '健康', '杂志', '明星写真', '美女', '图片', '波涛汹涌', '自卑', '是因为', '先天', '发育不良', '母体', '疾病', '胎儿', '发育', '影响', '之外', '父亲', '精子', '质量', '尿血', '无腹', '症状', '肾癌', '膀胱癌', '早期', '症状', '伴有', '腹疼', '尿血', '人体', '组织液', '含水量', '成年人', '体内', '含水量', '卷心菜', '医药', '资讯', '行业新闻', '健康', '媒体', '广告', '资源', '广告', '合作']\n",
      "8443164\n"
     ]
    }
   ],
   "source": [
    "# analyse whole phrase count, get top k as feature vector\n",
    "all_segs = []\n",
    "for k,v in dic_segs.items():\n",
    "    print(k, len(v), v[0])\n",
    "    for s in v:\n",
    "        all_segs.extend(s)\n",
    "print(len(all_segs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T15:32:04.754124Z",
     "start_time": "2018-12-02T15:32:02.990842Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264692\n",
      "[('使颅', 1), ('科季原', 1), ('巡犯', 1), ('寒凝', 1), ('喝靓', 1), ('白豆', 1), ('火调', 1), ('中温', 1), ('变嫩', 1), ('慢炖', 1), ('克钠', 1), ('愈短', 1), ('溃疡性', 1), ('糖皮质', 1), ('利萨', 1), ('刺梨', 1), ('护张', 1), ('频多', 1), ('低型', 1), ('麦芒', 1), ('扎般', 1), ('阴囊炎', 1), ('治癣', 1), ('眼源性', 1), ('向外看', 1), ('耳源性', 1), ('颈源性', 1), ('危象', 1), ('主动脉瓣', 1), ('苯妥英钠', 1), ('摄片', 1), ('电测', 1), ('心动图', 1), ('嘟噜', 1), ('理辩', 1), ('任中华', 1), ('大展宏图', 1), ('路遥', 1), ('记性差', 1), ('博钱', 1), ('长不出', 1), ('用则生', 1), ('生不生', 1), ('内径', 1), ('圆珠笔芯', 1), ('深里', 1), ('结合能', 1), ('茶不思', 1), ('力考', 1), ('完劲', 1)]\n",
      "[('中国', 45196), ('一个', 39981), ('公司', 34052), ('市场', 25656), ('企业', 22605), ('记者', 21930), ('信息', 20034), ('美国', 19860), ('国际', 17525), ('发展', 16016), ('工作', 15236), ('北京', 15080), ('时间', 14967), ('投资', 14662), ('情况', 14161), ('国家', 12319), ('治疗', 12048), ('服务', 11967), ('影响', 11786), ('价格', 11765), ('基金', 11496), ('相关', 11364), ('经济', 11318), ('技术', 10842), ('健康', 10825), ('考生', 10771), ('合作', 10579), ('地震', 10414), ('社会', 10321), ('产品', 10209), ('国内', 10138), ('项目', 10081), ('新闻', 9857), ('很多', 9674), ('上海', 9583), ('孩子', 9414), ('有限公司', 9364), ('提供', 9358), ('女性', 9262), ('位置', 9244), ('发生', 9182), ('灾区', 8973), ('投资者', 8594), ('特别', 8475), ('比赛', 8414), ('选择', 8400), ('包括', 8326), ('支持', 8222), ('集团', 8212), ('房地产', 8133)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "topk = sorted(dict(Counter(all_segs)).items(), key = lambda x:x[1], reverse = True)\n",
    "\n",
    "print(len(topk))\n",
    "print(topk[-50:])\n",
    "print(topk[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T15:32:09.773699Z",
     "start_time": "2018-12-02T15:32:09.726825Z"
    }
   },
   "outputs": [],
   "source": [
    "# 发现数量比较少的分词意义不是很大，这里选择前33000的词抽取为feature向量\n",
    "# delete the length 1 phrase\n",
    "topk = topk[:33000]\n",
    "l = len(topk)\n",
    "for i, tp in enumerate(topk[::-1]):\n",
    "    if len(tp[0]) <2:\n",
    "        topk.pop(l-1-i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T15:32:19.452040Z",
     "start_time": "2018-12-02T15:32:14.384371Z"
    }
   },
   "outputs": [],
   "source": [
    "# 删除非feature的分词\n",
    "import numpy as np\n",
    "set_words = set(  np.array(topk).T[0]  )\n",
    "for segs in dic_segs.values():\n",
    "    for s in segs:\n",
    "        for ss in s[::-1]:\n",
    "            if ss not in set_words:\n",
    "                s.remove(ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存分词结果语料库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T15:32:43.078333Z",
     "start_time": "2018-12-02T15:32:42.805982Z"
    }
   },
   "outputs": [],
   "source": [
    "xvec_raw = []\n",
    "ylabs = []\n",
    "for k,v in dic_segs.items():\n",
    "    ylabs.extend([k]*len(v))\n",
    "    xvec_raw.extend([ ' '.join(s) for s in v] )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T15:32:51.130720Z",
     "start_time": "2018-12-02T15:32:47.954213Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53765, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>客服 电话 空军 论坛 中国 飞机 西方 差距 真的 很大 阅读 回复 精华帖 发主帖数 累...</td>\n",
       "      <td>military</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>台湾 陆军 陆航 部队 直升机 空中 编队 台军 一年一度 汉光 计算机 兵棋 推演 晚间 ...</td>\n",
       "      <td>military</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>天美怡 首次 清除 酸毒 平衡 酸碱 途径 平衡 女性 荷尔蒙 体内 酸碱 平衡 弱碱 健康...</td>\n",
       "      <td>military</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>一行 离开 西藏 临行 交给 一封 斯汀 西藏 处于 中国 皇帝 统治 之下 中文 出生 印...</td>\n",
       "      <td>military</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>搭载 毫米 机炮 炮塔 垂直 障碍 试验 美国 德国 瑞士 意大利 兵器 爱好者 陌生 我国...</td>\n",
       "      <td>military</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0         1\n",
       "0  客服 电话 空军 论坛 中国 飞机 西方 差距 真的 很大 阅读 回复 精华帖 发主帖数 累...  military\n",
       "1  台湾 陆军 陆航 部队 直升机 空中 编队 台军 一年一度 汉光 计算机 兵棋 推演 晚间 ...  military\n",
       "2  天美怡 首次 清除 酸毒 平衡 酸碱 途径 平衡 女性 荷尔蒙 体内 酸碱 平衡 弱碱 健康...  military\n",
       "3  一行 离开 西藏 临行 交给 一封 斯汀 西藏 处于 中国 皇帝 统治 之下 中文 出生 印...  military\n",
       "4  搭载 毫米 机炮 炮塔 垂直 障碍 试验 美国 德国 瑞士 意大利 兵器 爱好者 陌生 我国...  military"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = pd.DataFrame([xvec_raw, ylabs])\n",
    "tmp = tmp.T\n",
    "print(tmp.shape)\n",
    "tmp.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T16:52:06.653861Z",
     "start_time": "2018-12-02T16:52:06.649873Z"
    }
   },
   "outputs": [],
   "source": [
    "del tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-01T07:37:28.111237Z",
     "start_time": "2018-12-01T07:37:27.136940Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "tmp.to_csv('raw_corpus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T17:01:51.952276Z",
     "start_time": "2018-12-02T17:01:51.654074Z"
    }
   },
   "outputs": [],
   "source": [
    "del all_segs\n",
    "del set_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型方法定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T08:17:23.826912Z",
     "start_time": "2018-11-27T08:17:07.692088Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T02:57:38.938553Z",
     "start_time": "2018-12-03T02:57:37.704833Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from time import time\n",
    "# from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T02:57:40.747195Z",
     "start_time": "2018-12-03T02:57:40.741209Z"
    }
   },
   "outputs": [],
   "source": [
    "def feature_extractor(input_x, case='tfidf', n_gram=(1,1)):\n",
    "    \"\"\"\n",
    "    特征抽取\n",
    "    :param corpus: \n",
    "    :param case: 不同的特征抽取方法\n",
    "    :return: \n",
    "    \"\"\"\n",
    "#     if n_gram == (1,1):\n",
    "#         if case.lower() == 'tfidf':\n",
    "#             return TfidfVectorizer().fit_transform(input_x)\n",
    "#         elif case.lower() == 'bagofwords':\n",
    "#             return CountVectorizer().fit_transform(input_x)\n",
    "#     else:\n",
    "    if case.lower() == 'tfidf':\n",
    "        return TfidfVectorizer(ngram_range=n_gram).fit_transform(input_x)\n",
    "    elif case.lower() == 'bagofwords':\n",
    "        return CountVectorizer(ngram_range=n_gram).fit_transform(input_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T02:57:41.099161Z",
     "start_time": "2018-12-03T02:57:41.093179Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_data_to_train_and_test(corpus, indices=0.2, random_state=10, shuffle=True):\n",
    "    \"\"\"\n",
    "    将数据划分为训练数据和测试数据\n",
    "    :param corpus: [input_x]\n",
    "    :param indices: 划分比例\n",
    "    :random_state: 随机种子\n",
    "    :param shuffle: 是否打乱数据\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    input_x, y = corpus\n",
    "    # 切分数据集\n",
    "    x_train, x_dev, y_train, y_dev = train_test_split(input_x, y, test_size=indices, random_state=random_state)\n",
    "    print(\"Vocabulary Size: {:d}\".format(input_x.shape[1]))\n",
    "    print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
    "    return x_train, x_dev, y_train, y_dev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T02:57:41.506068Z",
     "start_time": "2018-12-03T02:57:41.497061Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "alpha : float, optional (default=1.0)\n",
    "Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).\n",
    "\n",
    "fit_prior : boolean, optional (default=True)\n",
    "Whether to learn class prior probabilities or not. If false, a uniform prior will be used.\n",
    "\n",
    "class_prior : array-like, size (n_classes,), optional (default=None)\n",
    "Prior probabilities of the classes. If specified the priors are not adjusted according to the data.\n",
    "'''\n",
    "\n",
    "def fit_and_predicted(train_x, train_y, test_x, test_y):\n",
    "    \"\"\"\n",
    "    训练与预测\n",
    "    :param train_x: \n",
    "    :param train_y: \n",
    "    :param test_x: \n",
    "    :param test_y: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    clf = MultinomialNB().fit(train_x, train_y)\n",
    "    predicted = clf.predict(test_x)\n",
    "    print(metrics.classification_report(test_y, predicted))\n",
    "    print('accuracy_score: %0.5fs' %(metrics.accuracy_score(test_y, predicted)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T14:11:19.260346Z",
     "start_time": "2018-11-27T14:11:19.100581Z"
    }
   },
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T02:58:02.857325Z",
     "start_time": "2018-12-03T02:58:01.327802Z"
    }
   },
   "outputs": [],
   "source": [
    "# 读取分词结果数据\n",
    "\n",
    "import pandas as pd\n",
    "rawcorpus = pd.read_csv('raw_corpus.csv')\n",
    "rawcorpus = rawcorpus[rawcorpus['0'].notna()]\n",
    "xvec_raw = list(rawcorpus['0'])\n",
    "ylabs = list(rawcorpus['1'])\n",
    "del rawcorpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T02:58:15.435871Z",
     "start_time": "2018-12-03T02:58:08.365877Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 33003\n",
      "Train/Dev split: 42479/10620\n",
      "\t\t使用 bag-of-words 进行特征选择的朴素贝叶斯文本分类\t\t\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     culture       0.97      0.69      0.81       805\n",
      "         edu       0.93      0.90      0.92       935\n",
      "         ent       0.83      0.94      0.88       921\n",
      "     finance       0.78      0.87      0.82       969\n",
      "      health       0.89      0.81      0.85       885\n",
      "       house       0.93      0.88      0.90      1013\n",
      "        lady       0.79      0.88      0.84      1022\n",
      "    military       0.81      0.90      0.85      1019\n",
      "         nan       0.00      0.00      0.00         1\n",
      "        news       0.74      0.76      0.75      1037\n",
      "      sports       0.96      0.94      0.95       996\n",
      "        tech       0.84      0.80      0.82      1017\n",
      "\n",
      "   micro avg       0.85      0.85      0.85     10620\n",
      "   macro avg       0.79      0.78      0.78     10620\n",
      "weighted avg       0.86      0.85      0.85     10620\n",
      "\n",
      "accuracy_score: 0.85367s\n",
      "time uesed: 0.2490s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# n-gram 设置为1时候的训练结果\n",
    "xvec = feature_extractor(xvec_raw)    \n",
    "corpus = [xvec, ylabs]\n",
    "train_x, test_x, train_y, test_y = split_data_to_train_and_test(corpus)\n",
    "\n",
    "t0 = time()\n",
    "print('\\t\\t使用 bag-of-words 进行特征选择的朴素贝叶斯文本分类\\t\\t')\n",
    "fit_and_predicted(train_x, train_y, test_x, test_y)\n",
    "print('time uesed: %0.4fs' %(time() - t0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-01T05:28:23.294585Z",
     "start_time": "2018-12-01T05:24:01.370473Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " n_gram range %i - %i 1 2\n",
      "Vocabulary Size: 3405063\n",
      "Train/Dev split: 43012/10753\n",
      "\t\t使用 bag-of-words 进行特征选择的朴素贝叶斯文本分类\t\t\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     culture       0.99      0.61      0.76       791\n",
      "         edu       0.94      0.87      0.90       969\n",
      "         ent       0.87      0.89      0.88       967\n",
      "     finance       0.84      0.87      0.86      1052\n",
      "      health       0.94      0.80      0.86       929\n",
      "       house       0.95      0.89      0.92      1006\n",
      "        lady       0.78      0.90      0.83       994\n",
      "    military       0.71      0.95      0.81       961\n",
      "        news       0.77      0.82      0.79      1026\n",
      "      sports       0.98      0.95      0.97      1045\n",
      "        tech       0.85      0.86      0.85      1013\n",
      "\n",
      "   micro avg       0.86      0.86      0.86     10753\n",
      "   macro avg       0.87      0.85      0.86     10753\n",
      "weighted avg       0.87      0.86      0.86     10753\n",
      "\n",
      "accuracy_score: 0.85948s\n",
      "time uesed: 3.3462s\n",
      " n_gram range %i - %i 1 3\n",
      "Vocabulary Size: 8557918\n",
      "Train/Dev split: 43012/10753\n",
      "\t\t使用 bag-of-words 进行特征选择的朴素贝叶斯文本分类\t\t\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     culture       0.99      0.61      0.76       791\n",
      "         edu       0.94      0.88      0.91       969\n",
      "         ent       0.88      0.89      0.88       967\n",
      "     finance       0.85      0.87      0.86      1052\n",
      "      health       0.94      0.81      0.87       929\n",
      "       house       0.95      0.90      0.93      1006\n",
      "        lady       0.79      0.90      0.84       994\n",
      "    military       0.71      0.95      0.82       961\n",
      "        news       0.78      0.84      0.81      1026\n",
      "      sports       0.98      0.95      0.97      1045\n",
      "        tech       0.85      0.86      0.86      1013\n",
      "\n",
      "   micro avg       0.87      0.87      0.87     10753\n",
      "   macro avg       0.88      0.86      0.86     10753\n",
      "weighted avg       0.88      0.87      0.87     10753\n",
      "\n",
      "accuracy_score: 0.86515s\n",
      "time uesed: 5.5484s\n",
      " n_gram range %i - %i 1 4\n",
      "Vocabulary Size: 13993395\n",
      "Train/Dev split: 43012/10753\n",
      "\t\t使用 bag-of-words 进行特征选择的朴素贝叶斯文本分类\t\t\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     culture       1.00      0.61      0.76       791\n",
      "         edu       0.95      0.88      0.91       969\n",
      "         ent       0.88      0.89      0.89       967\n",
      "     finance       0.86      0.88      0.87      1052\n",
      "      health       0.94      0.82      0.87       929\n",
      "       house       0.95      0.91      0.93      1006\n",
      "        lady       0.80      0.90      0.85       994\n",
      "    military       0.71      0.95      0.82       961\n",
      "        news       0.79      0.86      0.82      1026\n",
      "      sports       0.98      0.95      0.97      1045\n",
      "        tech       0.85      0.86      0.86      1013\n",
      "\n",
      "   micro avg       0.87      0.87      0.87     10753\n",
      "   macro avg       0.88      0.86      0.87     10753\n",
      "weighted avg       0.88      0.87      0.87     10753\n",
      "\n",
      "accuracy_score: 0.86915s\n",
      "time uesed: 8.8872s\n"
     ]
    }
   ],
   "source": [
    "# 测试不同n-gram 参数下朴素贝叶斯分类器的分类效果\n",
    "\n",
    "for i in range(2, 5):    \n",
    "    print(' n_gram range %i - %i', 1, i)\n",
    "    xvec = feature_extractor(xvec_raw, case='tfidf', n_gram=(1,i))    \n",
    "    corpus = [xvec, ylabs]\n",
    "    train_x, test_x, train_y, test_y = split_data_to_train_and_test(corpus)\n",
    "\n",
    "    t0 = time()\n",
    "    print('\\t\\t使用 bag-of-words 进行特征选择的朴素贝叶斯文本分类\\t\\t')\n",
    "    fit_and_predicted(train_x, train_y, test_x, test_y)\n",
    "    print('time uesed: %0.4fs' %(time() - t0))\n",
    "\n",
    "    # 查看分类错误的几个？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-01T06:04:03.904923Z",
     "start_time": "2018-12-01T06:04:03.888003Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "def train_and_test_with_CV(corpus, cv=5, alpha=1, fit_prior=True):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    input_x, y = corpus\n",
    "    scoring = ['precision_macro', 'recall_macro', 'f1_macro', 'accuracy']\n",
    "    clf = MultinomialNB(alpha=alpha, fit_prior=fit_prior)\n",
    "    scores = cross_validate(clf, input_x, y, scoring=scoring,\n",
    "                            cv=cv, return_train_score=True)\n",
    "    return scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-01T06:07:45.997541Z",
     "start_time": "2018-12-01T06:05:29.999797Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([5.05995941, 5.03561258, 4.98289824, 5.03053784, 4.88753963]),\n",
       " 'score_time': array([3.16671181, 2.62435746, 2.58718896, 2.64961267, 2.55037355]),\n",
       " 'test_precision_macro': array([0.87355581, 0.87943749, 0.87266771, 0.8726302 , 0.87430351]),\n",
       " 'train_precision_macro': array([0.92638067, 0.9247399 , 0.92570231, 0.92693836, 0.92575978]),\n",
       " 'test_recall_macro': array([0.85838336, 0.86535583, 0.8550857 , 0.85664264, 0.85827054]),\n",
       " 'train_recall_macro': array([0.91684885, 0.91515884, 0.9161754 , 0.91772509, 0.9162749 ]),\n",
       " 'test_f1_macro': array([0.86066287, 0.86734092, 0.85732804, 0.85853156, 0.86049901]),\n",
       " 'train_f1_macro': array([0.91845514, 0.9167719 , 0.91779808, 0.91935296, 0.91791292]),\n",
       " 'test_accuracy': array([0.86274874, 0.86972289, 0.86003906, 0.86160714, 0.86300223]),\n",
       " 'train_accuracy': array([0.92069471, 0.91899747, 0.91995257, 0.92139586, 0.92000093])}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "input_x = feature_extractor(xvec_raw, case='tfidf', n_gram=(1,3))    \n",
    "scores = train_and_test_with_CV([input_x, ylabs])\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 寻找模型最佳参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-01T06:18:15.062772Z",
     "start_time": "2018-12-01T06:11:24.325626Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': [0, 1, 2, 4, 10], 'fit_prior': [True, False]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters:\n",
      "\talpha: 0\n",
      "\tclass_prior: None\n",
      "\tfit_prior: True\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "class sklearn.model_selection.GridSearchCV(estimator, param_grid, scoring=None, \n",
    "    fit_params=None, n_jobs=None, iid=’warn’, refit=True, cv=’warn’, verbose=0, pre_dispatch=‘2*n_jobs’, \n",
    "    error_score=’raise-deprecating’, return_train_score=’warn’)[source]¶\n",
    "Exhaustive search over specified parameter values for an estimator.\n",
    "\n",
    "Important members are fit, predict.\n",
    "\n",
    "GridSearchCV implements a “fit” and a “score” method. It also implements “predict”, “predict_proba”, “decision_function”, “transform” and “inverse_transform” if they are implemented in the estimator used.\n",
    "\n",
    "The parameters of the estimator used to apply these methods are optimized by cross-validated grid-search over a parameter grid.\n",
    "'''\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "def train_and_predicted_with_graid(corpus, cv, param_grid):\n",
    "    input_x, y = corpus\n",
    "    scoring = ['precision_macro', 'recall_macro', 'f1_macro']\n",
    "    clf = MultinomialNB()\n",
    "    grid = GridSearchCV(clf, param_grid, cv=cv, scoring='accuracy')\n",
    "\n",
    "    scores = grid.fit(input_x, y)\n",
    "\n",
    "    print('parameters:')\n",
    "    best_parameters = grid.best_estimator_.get_params()\n",
    "    for param_name in sorted(best_parameters):\n",
    "        print('\\t%s: %r' %(param_name, best_parameters[param_name]))\n",
    "    return scores\n",
    "\n",
    "\n",
    "k_alpha = [0, 1,2,4,10]\n",
    "fit_prior= [True, False]\n",
    "param_grid = dict(alpha=k_alpha, fit_prior=fit_prior)\n",
    "print(param_grid)\n",
    "scores = train_and_predicted_with_graid([input_x, ylabs], 5, param_grid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-01T06:22:29.093284Z",
     "start_time": "2018-12-01T06:22:29.087298Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'alpha': [0, 1, 2, 4, 10], 'fit_prior': [True, False]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T16:59:22.391031Z",
     "start_time": "2018-12-02T16:57:51.583047Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 8558149\n",
      "Train/Dev split: 43012/10753\n",
      "\t\t使用 tf-idf alpha = 0 进行特征选择的朴素贝叶斯文本分类\t\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     culture       0.88      0.88      0.88       834\n",
      "         edu       0.93      0.91      0.92      1037\n",
      "         ent       0.87      0.94      0.90       967\n",
      "     finance       0.83      0.86      0.85      1017\n",
      "      health       0.89      0.91      0.90       927\n",
      "       house       0.94      0.92      0.93       969\n",
      "        lady       0.89      0.84      0.86       979\n",
      "    military       0.90      0.87      0.89      1026\n",
      "        news       0.76      0.80      0.78      1013\n",
      "      sports       0.98      0.95      0.96       997\n",
      "        tech       0.85      0.81      0.83       987\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     10753\n",
      "   macro avg       0.88      0.88      0.88     10753\n",
      "weighted avg       0.88      0.88      0.88     10753\n",
      "\n",
      "accuracy_score: 0.88115s\n",
      "time uesed: 5.4448s\n"
     ]
    }
   ],
   "source": [
    "# 使用gridsearch 调的最优参数alpha = 0 进行训练\n",
    "xvec = feature_extractor(xvec_raw, case='tfidf', n_gram=(1,3))  \n",
    "corpus = [xvec, ylabs]\n",
    "train_x, test_x, train_y, test_y = split_data_to_train_and_test(corpus)\n",
    "t0 = time()\n",
    "print('\\t\\t使用 tf-idf alpha = 0 进行特征选择的朴素贝叶斯文本分类\\t\\t')\n",
    "clf = MultinomialNB(alpha=0, fit_prior=True, class_prior=None).fit(train_x, train_y)\n",
    "predicted = clf.predict(test_x)\n",
    "print(metrics.classification_report(test_y, predicted))\n",
    "print('accuracy_score: %0.5fs' %(metrics.accuracy_score(test_y, predicted)))\n",
    "print('time uesed: %0.4fs' %(time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T23:53:32.293182Z",
     "start_time": "2018-12-02T23:53:25.043171Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 33003\n",
      "Train/Dev split: 42479/10620\n"
     ]
    }
   ],
   "source": [
    "# xvec = feature_extractor(xvec_raw, case='tfidf', n_gram=(1,2))  \n",
    "xvec = feature_extractor(xvec_raw, case='tfidf', n_gram=(1,1))  \n",
    "corpus = [xvec, ylabs]\n",
    "train_x, test_x, train_y, test_y = split_data_to_train_and_test(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T00:19:10.340126Z",
     "start_time": "2018-12-03T00:18:45.063495Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     culture       0.83      0.79      0.81       805\n",
      "         edu       0.91      0.90      0.90       935\n",
      "         ent       0.77      0.82      0.80       921\n",
      "     finance       0.75      0.79      0.77       969\n",
      "      health       0.82      0.84      0.83       885\n",
      "       house       0.82      0.87      0.84      1013\n",
      "        lady       0.80      0.78      0.79      1022\n",
      "    military       0.80      0.83      0.82      1019\n",
      "         nan       0.00      0.00      0.00         1\n",
      "        news       0.93      0.86      0.89      1037\n",
      "      sports       0.91      0.88      0.90       996\n",
      "        tech       0.79      0.76      0.77      1017\n",
      "\n",
      "   micro avg       0.83      0.83      0.83     10620\n",
      "   macro avg       0.76      0.76      0.76     10620\n",
      "weighted avg       0.83      0.83      0.83     10620\n",
      "\n",
      "accuracy_score: 0.82891s\n",
      "time uesed: 25.2617s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, min_samples_split=12, \n",
    "                                    min_samples_leaf=13, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, \n",
    "                                    max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, \n",
    "                                    class_weight=None, presort=False)\n",
    "t0 = time()\n",
    "clf.fit(train_x, train_y)\n",
    "predicted = clf.predict(test_x)\n",
    "print(metrics.classification_report(test_y, predicted))\n",
    "print('accuracy_score: %0.5fs' %(metrics.accuracy_score(test_y, predicted)))\n",
    "print('time uesed: %0.4fs' %(time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xvec = feature_extractor(xvec_raw, case='tfidf', n_gram=(1,2))  \n",
    "xvec = feature_extractor(xvec_raw, case='tfidf', n_gram=(1,1))  \n",
    "corpus = [xvec, ylabs]\n",
    "train_x, test_x, train_y, test_y = split_data_to_train_and_test(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### base on MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T03:12:56.781262Z",
     "start_time": "2018-12-03T03:12:40.138254Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t使用 tf-idf 进行特征选择的AdaBoostClassifier base_estimator=MultinomialNB(alpha = 1.0e-10,\t\t\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     culture       0.67      0.89      0.76       805\n",
      "         edu       0.65      0.89      0.75       935\n",
      "         ent       0.87      0.76      0.81       921\n",
      "     finance       0.82      0.69      0.75       969\n",
      "      health       0.70      0.02      0.05       885\n",
      "       house       0.90      0.88      0.89      1013\n",
      "        lady       0.60      0.87      0.71      1022\n",
      "    military       0.72      0.71      0.71      1019\n",
      "         nan       0.00      0.00      0.00         1\n",
      "        news       0.67      0.07      0.12      1037\n",
      "      sports       0.77      0.53      0.63       996\n",
      "        tech       0.47      0.92      0.62      1017\n",
      "\n",
      "   micro avg       0.65      0.65      0.65     10620\n",
      "   macro avg       0.65      0.60      0.57     10620\n",
      "weighted avg       0.71      0.65      0.62     10620\n",
      "\n",
      "accuracy_score: 0.65499s\n",
      "time uesed: 1.1073s\n",
      "\t\t使用 tf-idf 进行特征选择的AdaBoostClassifier base_estimator=MultinomialNB(alpha = 1.0e-10,\t\t\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     culture       0.55      0.62      0.58       805\n",
      "         edu       0.59      0.84      0.69       935\n",
      "         ent       0.82      0.77      0.80       921\n",
      "     finance       0.83      0.71      0.77       969\n",
      "      health       0.15      0.07      0.10       885\n",
      "       house       0.77      0.92      0.84      1013\n",
      "        lady       0.55      0.77      0.64      1022\n",
      "    military       0.72      0.67      0.69      1019\n",
      "         nan       0.00      0.00      0.00         1\n",
      "        news       0.70      0.25      0.37      1037\n",
      "      sports       0.66      0.69      0.67       996\n",
      "        tech       0.72      0.77      0.74      1017\n",
      "\n",
      "   micro avg       0.65      0.65      0.65     10620\n",
      "   macro avg       0.59      0.59      0.57     10620\n",
      "weighted avg       0.65      0.65      0.63     10620\n",
      "\n",
      "accuracy_score: 0.64765s\n",
      "time uesed: 2.1180s\n",
      "\t\t使用 tf-idf 进行特征选择的AdaBoostClassifier base_estimator=MultinomialNB(alpha = 1.0e-10,\t\t\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     culture       0.43      0.36      0.39       805\n",
      "         edu       0.48      0.78      0.59       935\n",
      "         ent       0.67      0.79      0.73       921\n",
      "     finance       0.87      0.56      0.68       969\n",
      "      health       0.14      0.13      0.13       885\n",
      "       house       0.57      0.88      0.69      1013\n",
      "        lady       0.51      0.67      0.58      1022\n",
      "    military       0.69      0.67      0.68      1019\n",
      "         nan       0.00      0.00      0.00         1\n",
      "        news       0.92      0.04      0.08      1037\n",
      "      sports       0.57      0.70      0.63       996\n",
      "        tech       0.88      0.50      0.64      1017\n",
      "\n",
      "   micro avg       0.56      0.56      0.56     10620\n",
      "   macro avg       0.56      0.51      0.49     10620\n",
      "weighted avg       0.62      0.56      0.53     10620\n",
      "\n",
      "accuracy_score: 0.55744s\n",
      "time uesed: 2.8149s\n",
      "\t\t使用 tf-idf 进行特征选择的AdaBoostClassifier base_estimator=MultinomialNB(alpha = 1.0e-10,\t\t\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     culture       0.43      0.30      0.35       805\n",
      "         edu       0.49      0.49      0.49       935\n",
      "         ent       0.34      0.84      0.49       921\n",
      "     finance       0.90      0.27      0.42       969\n",
      "      health       0.07      0.14      0.09       885\n",
      "       house       0.47      0.85      0.61      1013\n",
      "        lady       0.53      0.58      0.55      1022\n",
      "    military       0.74      0.62      0.67      1019\n",
      "         nan       0.00      0.00      0.00         1\n",
      "        news       1.00      0.00      0.00      1037\n",
      "      sports       0.78      0.60      0.68       996\n",
      "        tech       0.94      0.17      0.29      1017\n",
      "\n",
      "   micro avg       0.45      0.45      0.45     10620\n",
      "   macro avg       0.56      0.41      0.39     10620\n",
      "weighted avg       0.62      0.45      0.43     10620\n",
      "\n",
      "accuracy_score: 0.44520s\n",
      "time uesed: 4.6852s\n",
      "\t\t使用 tf-idf 进行特征选择的AdaBoostClassifier base_estimator=MultinomialNB(alpha = 1.0e-10,\t\t\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     culture       0.38      0.22      0.28       805\n",
      "         edu       0.61      0.39      0.48       935\n",
      "         ent       0.25      0.83      0.39       921\n",
      "     finance       0.94      0.12      0.22       969\n",
      "      health       0.18      0.57      0.28       885\n",
      "       house       0.45      0.79      0.58      1013\n",
      "        lady       0.55      0.41      0.47      1022\n",
      "    military       0.79      0.48      0.60      1019\n",
      "         nan       0.00      0.00      0.00         1\n",
      "        news       0.00      0.00      0.00      1037\n",
      "      sports       0.96      0.43      0.60       996\n",
      "        tech       1.00      0.05      0.09      1017\n",
      "\n",
      "   micro avg       0.39      0.39      0.39     10620\n",
      "   macro avg       0.51      0.36      0.33     10620\n",
      "weighted avg       0.56      0.39      0.36     10620\n",
      "\n",
      "accuracy_score: 0.38748s\n",
      "time uesed: 5.7659s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "for i in [4,7, 10, 15, 19]:\n",
    "    t0 = time()\n",
    "    print('\\t\\t使用 tf-idf 进行特征选择的AdaBoostClassifier base_estimator=MultinomialNB(alpha = 1.0e-10,\\t\\t')\n",
    "    clf = AdaBoostClassifier(base_estimator=MultinomialNB(alpha = 1.0e-10, fit_prior=True, class_prior=None), n_estimators=i, \n",
    "                             learning_rate=0.77, algorithm='SAMME.R', random_state=None).fit(train_x, train_y)\n",
    "    predicted = clf.predict(test_x)\n",
    "    print(metrics.classification_report(test_y, predicted))\n",
    "    print('accuracy_score: %0.5fs' %(metrics.accuracy_score(test_y, predicted)))\n",
    "    print('time uesed: %0.4fs' %(time() - t0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置learning rate 比较低是时候效果较好。 总体不如直接使用NB 算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T03:16:12.891089Z",
     "start_time": "2018-12-03T03:15:51.455422Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t使用 tf-idf 进行特征选择的AdaBoostClassifier base_estimator=MultinomialNB(alpha = 1.0e-10,\t\t\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     culture       0.94      0.70      0.80       805\n",
      "         edu       0.96      0.83      0.89       935\n",
      "         ent       0.91      0.74      0.82       921\n",
      "     finance       0.81      0.62      0.70       969\n",
      "      health       0.54      0.95      0.69       885\n",
      "       house       0.97      0.61      0.75      1013\n",
      "        lady       0.91      0.66      0.76      1022\n",
      "    military       0.87      0.70      0.77      1019\n",
      "         nan       0.00      0.00      0.00         1\n",
      "        news       0.82      0.51      0.63      1037\n",
      "      sports       0.92      0.81      0.87       996\n",
      "        tech       0.38      0.93      0.54      1017\n",
      "\n",
      "   micro avg       0.73      0.73      0.73     10620\n",
      "   macro avg       0.75      0.67      0.69     10620\n",
      "weighted avg       0.82      0.73      0.75     10620\n",
      "\n",
      "accuracy_score: 0.72900s\n",
      "time uesed: 7.9647s\n",
      "\t\t使用 tf-idf 进行特征选择的AdaBoostClassifier base_estimator=MultinomialNB(alpha = 1.0e-10,\t\t\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     culture       0.94      0.69      0.79       805\n",
      "         edu       0.96      0.82      0.88       935\n",
      "         ent       0.90      0.72      0.80       921\n",
      "     finance       0.82      0.57      0.67       969\n",
      "      health       0.53      0.95      0.68       885\n",
      "       house       0.97      0.60      0.74      1013\n",
      "        lady       0.92      0.65      0.76      1022\n",
      "    military       0.88      0.65      0.75      1019\n",
      "         nan       0.00      0.00      0.00         1\n",
      "        news       0.81      0.49      0.61      1037\n",
      "      sports       0.92      0.76      0.83       996\n",
      "        tech       0.35      0.93      0.51      1017\n",
      "\n",
      "   micro avg       0.71      0.71      0.71     10620\n",
      "   macro avg       0.75      0.65      0.67     10620\n",
      "weighted avg       0.82      0.71      0.73     10620\n",
      "\n",
      "accuracy_score: 0.70791s\n",
      "time uesed: 13.4610s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "for i in [30, 50]:\n",
    "    t0 = time()\n",
    "    print('\\t\\t使用 tf-idf 进行特征选择的AdaBoostClassifier base_estimator=MultinomialNB(alpha = 1.0e-10,\\t\\t')\n",
    "    clf = AdaBoostClassifier(base_estimator=MultinomialNB(alpha = 1.0e-10, fit_prior=True, class_prior=None), n_estimators=i, \n",
    "                             learning_rate=0.2, algorithm='SAMME.R', random_state=None).fit(train_x, train_y)\n",
    "    predicted = clf.predict(test_x)\n",
    "    print(metrics.classification_report(test_y, predicted))\n",
    "    print('accuracy_score: %0.5fs' %(metrics.accuracy_score(test_y, predicted)))\n",
    "    print('time uesed: %0.4fs' %(time() - t0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### base on DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-03T00:22:01.983Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t使用 tf-idf 进行特征选择的AdaBoostClassifier base_estimator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     culture       0.93      0.82      0.87       805\n",
      "         edu       0.97      0.91      0.94       935\n",
      "         ent       0.79      0.93      0.85       921\n",
      "     finance       0.80      0.85      0.83       969\n",
      "      health       0.90      0.86      0.88       885\n",
      "       house       0.96      0.82      0.88      1013\n",
      "        lady       0.84      0.87      0.85      1022\n",
      "    military       0.86      0.89      0.88      1019\n",
      "         nan       0.00      0.00      0.00         1\n",
      "        news       0.96      0.86      0.90      1037\n",
      "      sports       0.96      0.92      0.94       996\n",
      "        tech       0.72      0.86      0.78      1017\n",
      "\n",
      "   micro avg       0.87      0.87      0.87     10620\n",
      "   macro avg       0.81      0.80      0.80     10620\n",
      "weighted avg       0.88      0.87      0.87     10620\n",
      "\n",
      "accuracy_score: 0.87119s\n",
      "time uesed: 2034.9170s\n",
      "\t\t使用 tf-idf 进行特征选择的AdaBoostClassifier base_estimator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     culture       0.89      0.84      0.86       805\n",
      "         edu       0.97      0.90      0.93       935\n",
      "         ent       0.79      0.94      0.86       921\n",
      "     finance       0.80      0.86      0.83       969\n",
      "      health       0.91      0.87      0.89       885\n",
      "       house       0.97      0.82      0.89      1013\n",
      "        lady       0.84      0.87      0.86      1022\n",
      "    military       0.87      0.87      0.87      1019\n",
      "         nan       0.00      0.00      0.00         1\n",
      "        news       0.97      0.86      0.91      1037\n",
      "      sports       0.96      0.91      0.94       996\n",
      "        tech       0.73      0.87      0.80      1017\n",
      "\n",
      "   micro avg       0.87      0.87      0.87     10620\n",
      "   macro avg       0.81      0.80      0.80     10620\n",
      "weighted avg       0.88      0.87      0.88     10620\n",
      "\n",
      "accuracy_score: 0.87382s\n",
      "time uesed: 3231.3217s\n",
      "\t\t使用 tf-idf 进行特征选择的AdaBoostClassifier base_estimator\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "for i in [ 50, 77, 100]:\n",
    "    t0 = time()\n",
    "    print('\\t\\t使用 tf-idf 进行特征选择的AdaBoostClassifier base_estimator')\n",
    "    clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, min_samples_split=12, \n",
    "                                    min_samples_leaf=13, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, \n",
    "                                    max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, \n",
    "                                    class_weight=None, presort=False), \n",
    "                             n_estimators=i, \n",
    "                             learning_rate=0.7, algorithm='SAMME.R', random_state=None).fit(train_x, train_y)\n",
    "    predicted = clf.predict(test_x)\n",
    "    print(metrics.classification_report(test_y, predicted))\n",
    "    print('accuracy_score: %0.5fs' %(metrics.accuracy_score(test_y, predicted)))\n",
    "    print('time uesed: %0.4fs' %(time() - t0))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM 分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-01T07:38:00.864654Z",
     "start_time": "2018-12-01T07:38:00.591090Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score , roc_auc_score , roc_curve\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-01T07:38:16.568432Z",
     "start_time": "2018-12-01T07:38:08.777065Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53765, 33000)\n"
     ]
    }
   ],
   "source": [
    "xvec = feature_extractor(xvec_raw, n_gram=(1,1))   \n",
    "print(xvec.shape)\n",
    "corpus = [xvec, ylabs]\n",
    "# train_x, test_x, train_y, test_y = split_data_to_train_and_test(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-01T07:54:23.125320Z",
     "start_time": "2018-12-01T07:38:21.769323Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 33000\n",
      "Train/Dev split: 43012/10753\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     culture       0.96      0.87      0.91       801\n",
      "         edu       0.94      0.89      0.92       969\n",
      "         ent       0.77      0.96      0.85       981\n",
      "     finance       0.86      0.86      0.86       967\n",
      "      health       0.90      0.91      0.90       933\n",
      "       house       0.94      0.95      0.94      1013\n",
      "        lady       0.88      0.85      0.87      1028\n",
      "    military       0.88      0.95      0.91      1026\n",
      "        news       0.89      0.81      0.85      1041\n",
      "      sports       0.96      0.95      0.95       981\n",
      "        tech       0.89      0.85      0.87      1013\n",
      "\n",
      "   micro avg       0.89      0.89      0.89     10753\n",
      "   macro avg       0.90      0.89      0.89     10753\n",
      "weighted avg       0.90      0.89      0.89     10753\n",
      "\n",
      "accuracy_score: 0.89417s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_x, test_x, train_y, test_y = split_data_to_train_and_test(corpus)\n",
    "\n",
    "#C: 目标函数的惩罚系数C，用来平衡分类间隔margin和错分样本的，default C = 1.0\n",
    "#kernel：参数选择有rbf, linear, poly, Sigmoid, 默认的是\"RBF\";\n",
    "svmmodel_fit = SVC(C = 1.0 , kernel= \"linear\").fit(train_x, train_y) \n",
    "print(svmmodel_fit) #12mins 2018-12-1 15:50:35\n",
    "predicted = svmmodel_fit.predict(test_x)\n",
    "print(metrics.classification_report(test_y, predicted))\n",
    "print('accuracy_score: %0.5fs' %(metrics.accuracy_score(test_y, predicted)))\n",
    "\n",
    "#  finished 15:54:23 2018-12-01 ； 预测5mins 左右\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# temp 问题 buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T17:46:47.354768Z",
     "start_time": "2018-12-02T17:46:47.346822Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "# 为什么会出现na？？\n",
    "for x in xvec_raw[:]:\n",
    "    if not isinstance(x, str):\n",
    "        print(x)\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(deleteNs, test_accuracy_list)\n",
    "plt.title('Relationship of deleteNs and test_accuracy')\n",
    "plt.xlabel('deleteNs')\n",
    "plt.ylabel('test_accuracy')\n",
    "plt.savefig('result.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "197px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 473,
   "position": {
    "height": "495px",
    "left": "776px",
    "right": "20px",
    "top": "59px",
    "width": "477px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
