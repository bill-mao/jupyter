{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "quickbrownfox = \"A quick brown fox jumps over the lazy dog.\"\n",
    "split_regex = r'\\W+'\n",
    "\n",
    "# TODO Implement this\n",
    "def simple_tokenize(string):\n",
    "    string= string.lower()\n",
    "    string = string.strip(r'\\W')\n",
    "    p = re.compile(split_regex,re.I) #根据空格或者数字拆分字符串\n",
    "    list = p.split(string)\n",
    "    if(list[0]== ''):\n",
    "        del list[0]\n",
    "    length = len(list)-1\n",
    "    if(list[length]== ''):\n",
    "        del list[length]\n",
    "    return list\n",
    "        \n",
    "\n",
    "print simple_tokenize(quickbrownfox) # Should give ['a', 'quick', 'brown', ... ]\n",
    "\n",
    "# Simple testcases\n",
    "assert(simple_tokenize(\" \") == [])\n",
    "assert(simple_tokenize(\"!!!!123A/456_B/789C.123A\") == [\"123a\",\"456_b\",\"789c\",\"123a\"])\n",
    "assert(simple_tokenize(quickbrownfox) == ['a','quick','brown','fox','jumps','over','the','lazy','dog'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog']\n",
      "[] ['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "stopwords = [] # Load from file\n",
    "DATA_PATH = \"/home/bill/entity evolution/data\"\n",
    "with open(DATA_PATH+r\"/stopwords.txt\",'r') as file:\n",
    "    stopwords=file.read()\n",
    "stopwords=stopwords.split('\\n')\n",
    "print type(stopwords)\n",
    "\n",
    "# TODO Implement this\n",
    "\n",
    "def tokenize(string):\n",
    "    list1 = simple_tokenize(string)\n",
    "    for token in  stopwords:\n",
    "        while token in list1:\n",
    "            list1.remove(token)\n",
    "    return list1\n",
    "\n",
    "print tokenize(quickbrownfox) # Should give ['quick', 'brown', ... ]\n",
    "#print tokenize(\"Why a the if again no ?\") , tokenize(quickbrownfox) \n",
    "\n",
    "assert(tokenize(\"Why a the?\") == [])\n",
    "assert(tokenize(quickbrownfox) == ['quick','brown','fox','jumps','lazy','dog'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c. Now let's tokenize the two small data sets. For each one build a dictionary of tokens, i.e., \n",
    "a dictionary where the record IDs are the keys, and the output of tokenize is the values.\n",
    "Include tokens for the name, description,and manufacturer fields, but not the price field. \n",
    "How many tokens, total, are there in the two data sets? Which Amazon record has the biggest number of tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 tokens in the combined data sets\n",
      "The Amazon record with ID \"\" has the most tokens\n"
     ]
    }
   ],
   "source": [
    "# TODO Compute these (dict() or DataFrame OK)\n",
    "import pandas as pd\n",
    "PATH= \"/home/bill/dataScience/entity evolution/data\"\n",
    "amazon_df = pd.read_csv(PATH+r\"/Amazon.csv\")\n",
    "google_df = pd.read_csv(PATH+r\"/Google.csv\")\n",
    "\n",
    "\n",
    "\n",
    "amazon_rec2tok = {}\n",
    "google_rec2tok = {}\n",
    "\n",
    "total_tokens = 0 # TODO Fix me\n",
    "print 'There are %s tokens in the combined data sets' % total_tokens\n",
    "\n",
    "biggest_record = \"\" # TODO Fix me\n",
    "print 'The Amazon record with ID \"%s\" has the most tokens' % biggest_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
